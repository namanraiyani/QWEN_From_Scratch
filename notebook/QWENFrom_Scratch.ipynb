{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Installation","metadata":{}},{"cell_type":"code","source":"!pip install blobfile>=3.0.0 huggingface_hub>=0.24.7 ipywidgets>=8.1.2 safetensors>=0.4.4 sentencepiece>=0.1.99","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:14.951323Z","iopub.execute_input":"2025-07-29T13:08:14.951605Z","iopub.status.idle":"2025-07-29T13:08:18.213202Z","shell.execute_reply.started":"2025-07-29T13:08:14.951587Z","shell.execute_reply":"2025-07-29T13:08:18.212330Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"### Check versions of important libraries","metadata":{}},{"cell_type":"code","source":"from importlib.metadata import version\n\n# check versions of needed libs\nlibs = [\n    \"huggingface_hub\",  # downloads pretrained weights from hf\n    \"tokenizers\",       # handles text-to-tokens conversion  \n    \"torch\",           # main ml framework\n]\nfor lib in libs:\n    print(f\"{lib} version: {version(lib)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:18.214853Z","iopub.execute_input":"2025-07-29T13:08:18.215109Z","iopub.status.idle":"2025-07-29T13:08:18.224493Z","shell.execute_reply.started":"2025-07-29T13:08:18.215085Z","shell.execute_reply":"2025-07-29T13:08:18.223927Z"}},"outputs":[{"name":"stdout","text":"huggingface_hub version: 0.33.1\ntokenizers version: 0.21.2\ntorch version: 2.6.0+cu124\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"### Choose between reasoning model and base model","metadata":{}},{"cell_type":"code","source":"USE_REASONING = True  # whether to use reasoning model or base","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:18.225363Z","iopub.execute_input":"2025-07-29T13:08:18.225605Z","iopub.status.idle":"2025-07-29T13:08:18.238836Z","shell.execute_reply.started":"2025-07-29T13:08:18.225582Z","shell.execute_reply":"2025-07-29T13:08:18.238234Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"### Define a feedforward network with gating and SwiGLU activation","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass FeedForward(nn.Module):\n    # simple feedforward network with gating mechanisim\n    def __init__(self, cfg):\n        super().__init__()\n        # 3 linear layers: 2 for gating, 1 for output\n        self.gate = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)  \n        self.up = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n        self.down = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n\n    def forward(self, x):\n        # swiglu activation: silu(gate) * up_proj\n        gate_out = self.gate(x)\n        up_out = self.up(x) \n        activated = nn.functional.silu(gate_out) * up_out  # element-wise multiply\n        return self.down(activated)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:18.240352Z","iopub.execute_input":"2025-07-29T13:08:18.240565Z","iopub.status.idle":"2025-07-29T13:08:18.252602Z","shell.execute_reply.started":"2025-07-29T13:08:18.240550Z","shell.execute_reply":"2025-07-29T13:08:18.251889Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"### RMSNorm: an alternative to LayerNorm with optional Qwen3 compatibility","metadata":{}},{"cell_type":"code","source":"class RMSNorm(nn.Module):\n    # root mean square normaliztion - alternative to layernorm\n    def __init__(self, dim, eps=1e-6, bias=False, qwen3_compat=True):\n        super().__init__()\n        self.eps = eps  # small value to avoid division by zero\n        self.qwen3_compat = qwen3_compat  # compatibility flag\n        self.weight = nn.Parameter(torch.ones(dim))  # learnable scale\n        self.bias = nn.Parameter(torch.zeros(dim)) if bias else None  # optional bias\n\n    def forward(self, x):\n        orig_dtype = x.dtype\n        \n        # convert to float32 for numerical stability\n        if self.qwen3_compat:\n            x = x.to(torch.float32)\n\n        # compute rms and normalize\n        var = x.pow(2).mean(dim=-1, keepdim=True)  # mean of squares\n        normed = x * torch.rsqrt(var + self.eps)   # x / sqrt(variance)\n        normed = normed * self.weight              # scale\n\n        if self.bias is not None:\n            normed = normed + self.bias  # shift if bias exists\n\n        return normed.to(orig_dtype)  # back to original dtype","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:18.253386Z","iopub.execute_input":"2025-07-29T13:08:18.253578Z","iopub.status.idle":"2025-07-29T13:08:18.266569Z","shell.execute_reply.started":"2025-07-29T13:08:18.253562Z","shell.execute_reply":"2025-07-29T13:08:18.265880Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"### Compute rotary position encodings (RoPE) for attention mechanism","metadata":{}},{"cell_type":"code","source":"def compute_rope_freqs(head_dim, base=10_000, max_len=4096, dtype=torch.float32):\n    # rotary position encoding - helps model understand position\n    assert head_dim % 2 == 0, \"head dim must be even for rope\"\n\n    # compute inverse frequencies for each dimension pair\n    inv_freqs = 1.0 / (base ** (torch.arange(0, head_dim, 2, dtype=dtype)[:(head_dim // 2)].float() / head_dim))\n    \n    # position indices from 0 to max_len-1\n    pos = torch.arange(max_len, dtype=dtype)\n    \n    # compute angles: pos * inv_freq for each position-frequency pair\n    angles = pos[:, None] * inv_freqs[None, :]  # shape: (max_len, head_dim//2)\n    \n    # duplicate angles to match full head dimension\n    angles = torch.cat([angles, angles], dim=1)  # shape: (max_len, head_dim)\n    \n    # precompute cos and sin for efficiency\n    cos_vals = torch.cos(angles)\n    sin_vals = torch.sin(angles)\n    \n    return cos_vals, sin_vals","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:18.267373Z","iopub.execute_input":"2025-07-29T13:08:18.267538Z","iopub.status.idle":"2025-07-29T13:08:18.284814Z","shell.execute_reply.started":"2025-07-29T13:08:18.267525Z","shell.execute_reply":"2025-07-29T13:08:18.284202Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"### Rotary position encodings (RoPE) to attention input tensor","metadata":{}},{"cell_type":"code","source":"def apply_rope(x, cos_vals, sin_vals):\n    # applies rotary encoding to input tensor\n    # x shape: (batch, heads, seq_len, head_dim)\n    b, h, seq_len, d = x.shape\n    assert d % 2 == 0, \"head dim must be even\"\n    \n    # split into two halves for rotation\n    x1 = x[..., :d//2]   # first half\n    x2 = x[..., d//2:]   # second half\n    \n    # adjust cos/sin shapes to match input\n    cos_vals = cos_vals[:seq_len, :].unsqueeze(0).unsqueeze(0)  # (1,1,seq_len,head_dim)\n    sin_vals = sin_vals[:seq_len, :].unsqueeze(0).unsqueeze(0)\n    \n    # rotation: combine original and rotated components\n    rotated = torch.cat((-x2, x1), dim=-1)  # rotate by 90 degrees\n    result = (x * cos_vals) + (rotated * sin_vals)\n    \n    return result.to(dtype=x.dtype)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:18.285363Z","iopub.execute_input":"2025-07-29T13:08:18.285531Z","iopub.status.idle":"2025-07-29T13:08:18.299973Z","shell.execute_reply.started":"2025-07-29T13:08:18.285518Z","shell.execute_reply":"2025-07-29T13:08:18.299313Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"### Grouped query attention layer that shares key/value projections across head groups to save memory","metadata":{}},{"cell_type":"code","source":"class GroupedQueryAttention(nn.Module):\n    # grouped query attention - saves memory by sharing k,v across heads\n    def __init__(self, d_in, n_heads, n_kv_groups, head_dim=None, qk_norm=False, dtype=None):\n        super().__init__()\n        assert n_heads % n_kv_groups == 0, \"heads must be divisible by kv groups\"\n        \n        self.n_heads = n_heads\n        self.n_kv_groups = n_kv_groups  \n        self.group_size = n_heads // n_kv_groups  # how many q heads per kv head\n        \n        # calculate head dimension if not provided\n        if head_dim is None:\n            assert d_in % n_heads == 0, \"d_in must divide evenly by n_heads\"\n            head_dim = d_in // n_heads\n            \n        self.head_dim = head_dim\n        self.d_out = n_heads * head_dim\n        \n        # projection layers\n        self.q_proj = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n        self.k_proj = nn.Linear(d_in, n_kv_groups * head_dim, bias=False, dtype=dtype) \n        self.v_proj = nn.Linear(d_in, n_kv_groups * head_dim, bias=False, dtype=dtype)\n        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n        \n        # optional query/key normalization\n        if qk_norm:\n            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n        else:\n            self.q_norm = self.k_norm = None\n\n    def forward(self, x, mask, cos_vals, sin_vals):\n        b, seq_len, _ = x.shape\n        \n        # project to q, k, v\n        q = self.q_proj(x)  # (b, seq_len, n_heads * head_dim)\n        k = self.k_proj(x)  # (b, seq_len, n_kv_groups * head_dim)  \n        v = self.v_proj(x)  # (b, seq_len, n_kv_groups * head_dim)\n        \n        # reshape to separate heads\n        q = q.view(b, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n        k = k.view(b, seq_len, self.n_kv_groups, self.head_dim).transpose(1, 2)\n        v = v.view(b, seq_len, self.n_kv_groups, self.head_dim).transpose(1, 2)\n        \n        # apply normalization if enabled\n        if self.q_norm:\n            q = self.q_norm(q)\n        if self.k_norm:\n            k = self.k_norm(k)\n            \n        # apply rotary position encoding\n        q = apply_rope(q, cos_vals, sin_vals)\n        k = apply_rope(k, cos_vals, sin_vals)\n        \n        # expand k,v to match number of query heads\n        k = k.repeat_interleave(self.group_size, dim=1)\n        v = v.repeat_interleave(self.group_size, dim=1)\n        \n        # compute attention scores and apply mask\n        scores = q @ k.transpose(2, 3)  # (b, heads, seq_len, seq_len)\n        scores = scores.masked_fill(mask, -torch.inf)  # mask future tokens\n        weights = torch.softmax(scores / self.head_dim**0.5, dim=-1)  # scale and softmax\n        \n        # apply attention weights to values\n        out = (weights @ v).transpose(1, 2).reshape(b, seq_len, self.d_out)\n        return self.out_proj(out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:18.300562Z","iopub.execute_input":"2025-07-29T13:08:18.300758Z","iopub.status.idle":"2025-07-29T13:08:18.318313Z","shell.execute_reply.started":"2025-07-29T13:08:18.300707Z","shell.execute_reply":"2025-07-29T13:08:18.317744Z"}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":"### Single transformer block combining grouped attention and feedforward with RMS normalization and residual connections","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    # single transformer layer with attention + feedforward\n    def __init__(self, cfg):\n        super().__init__()\n        self.attn = GroupedQueryAttention(\n            d_in=cfg[\"emb_dim\"],\n            n_heads=cfg[\"n_heads\"], \n            head_dim=cfg[\"head_dim\"],\n            n_kv_groups=cfg[\"n_kv_groups\"],\n            qk_norm=cfg[\"qk_norm\"],\n            dtype=cfg[\"dtype\"]\n        )\n        self.ff = FeedForward(cfg)\n        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)  # pre-attention norm\n        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)  # pre-ff norm\n\n    def forward(self, x, mask, cos_vals, sin_vals):\n        # attention block with residual connection\n        residual = x\n        x = self.norm1(x)  # pre-norm\n        x = self.attn(x, mask, cos_vals, sin_vals) \n        x = x + residual   # residual connection\n        \n        # feedforward block with residual connection  \n        residual = x\n        x = self.norm2(x)  # pre-norm\n        x = self.ff(x)\n        x = x + residual   # residual connection\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:18.319088Z","iopub.execute_input":"2025-07-29T13:08:18.319472Z","iopub.status.idle":"2025-07-29T13:08:18.336779Z","shell.execute_reply.started":"2025-07-29T13:08:18.319449Z","shell.execute_reply":"2025-07-29T13:08:18.336062Z"}},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":"### Full transformer model with token embedding, stacked transformer blocks, rotary embeddings, and output projection","metadata":{}},{"cell_type":"code","source":"class Qwen3Model(nn.Module):\n    # main transformer model\n    def __init__(self, cfg):\n        super().__init__()\n        \n        # embedding layer converts tokens to vectors\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n        \n        # stack of transformer blocks\n        self.layers = nn.ModuleList([\n            TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])\n        ])\n        \n        # final norm and output projection\n        self.final_norm = RMSNorm(cfg[\"emb_dim\"]) \n        self.lm_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n        \n        # precompute rope frequencies\n        if cfg[\"head_dim\"] is None:\n            hd = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n        else:\n            hd = cfg[\"head_dim\"]\n            \n        cos_vals, sin_vals = compute_rope_freqs(\n            head_dim=hd,\n            base=cfg[\"rope_base\"], \n            max_len=cfg[\"context_length\"]\n        )\n        # register as buffers so they move with model to gpu/cpu\n        self.register_buffer(\"cos_vals\", cos_vals, persistent=False)\n        self.register_buffer(\"sin_vals\", sin_vals, persistent=False)\n        self.cfg = cfg\n\n    def forward(self, token_ids):\n        # convert tokens to embeddings\n        x = self.tok_emb(token_ids)\n        \n        # create causal mask to prevent looking at future tokens\n        seq_len = x.shape[1]\n        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device, dtype=torch.bool), diagonal=1)\n        \n        # pass through all transformer layers\n        for layer in self.layers:\n            x = layer(x, mask, self.cos_vals, self.sin_vals)\n            \n        # final normalization and projection to vocab\n        x = self.final_norm(x) \n        logits = self.lm_head(x.to(self.cfg[\"dtype\"]))\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:18.339274Z","iopub.execute_input":"2025-07-29T13:08:18.339473Z","iopub.status.idle":"2025-07-29T13:08:18.353536Z","shell.execute_reply.started":"2025-07-29T13:08:18.339447Z","shell.execute_reply":"2025-07-29T13:08:18.352865Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"### Model size configurations with hyperparameters for different parameter scales","metadata":{}},{"cell_type":"code","source":"# model size configuration\nMODEL_SIZE = \"0.6B\"  # can be 0.6B, 1.7B, 4B, 8B, 14B, 32B\n\n# config dicts for different model sizes\nif MODEL_SIZE == \"0.6B\":\n    CONFIG = {\n        \"vocab_size\": 151_936,      # number of tokens in vocabulary\n        \"context_length\": 40_960,   # max sequence length during training\n        \"emb_dim\": 1024,           # embedding/hidden dimension\n        \"n_heads\": 16,             # attention heads\n        \"n_layers\": 28,            # transformer layers\n        \"hidden_dim\": 3072,        # feedforward hidden size\n        \"head_dim\": 128,           # dimension per attention head\n        \"qk_norm\": True,           # normalize queries and keys\n        \"n_kv_groups\": 8,          # kv groups for grouped attention\n        \"rope_base\": 1_000_000.0,  # rope frequency base\n        \"dtype\": torch.bfloat16,   # reduced precision for memory\n    }\nelif MODEL_SIZE == \"1.7B\":\n    CONFIG = {\n        \"vocab_size\": 151_936,\n        \"context_length\": 40_960,\n        \"emb_dim\": 2048,           # 2x bigger than 0.6B\n        \"n_heads\": 16,\n        \"n_layers\": 28, \n        \"hidden_dim\": 6144,        # 2x bigger\n        \"head_dim\": 128,\n        \"qk_norm\": True,\n        \"n_kv_groups\": 8,\n        \"rope_base\": 1_000_000.0,\n        \"dtype\": torch.bfloat16,\n    }\nelif MODEL_SIZE == \"4B\":\n    CONFIG = {\n        \"vocab_size\": 151_936,\n        \"context_length\": 40_960,\n        \"emb_dim\": 2560,           # 25% bigger than 1.7B\n        \"n_heads\": 32,             # 2x more heads\n        \"n_layers\": 36,            # 29% more layers  \n        \"hidden_dim\": 9728,        # ~3x bigger ff\n        \"head_dim\": 128,\n        \"qk_norm\": True,\n        \"n_kv_groups\": 8,\n        \"rope_base\": 1_000_000.0,\n        \"dtype\": torch.bfloat16,\n    }\nelif MODEL_SIZE == \"8B\":\n    CONFIG = {\n        \"vocab_size\": 151_936,\n        \"context_length\": 40_960,\n        \"emb_dim\": 4096,           # 60% bigger than 4B\n        \"n_heads\": 32,\n        \"n_layers\": 36,            # 26% more layers\n        \"hidden_dim\": 12288,\n        \"head_dim\": 128,\n        \"qk_norm\": True,\n        \"n_kv_groups\": 8,\n        \"rope_base\": 1_000_000.0,\n        \"dtype\": torch.bfloat16,\n    }\nelif MODEL_SIZE == \"14B\":\n    CONFIG = {\n        \"vocab_size\": 151_936,\n        \"context_length\": 40_960,\n        \"emb_dim\": 5120,           # 25% bigger than 8B\n        \"n_heads\": 40,             # 25% more heads\n        \"n_layers\": 40,            # 11% more layers\n        \"hidden_dim\": 17408,       # 42% bigger ff\n        \"head_dim\": 128,\n        \"qk_norm\": True,\n        \"n_kv_groups\": 8,\n        \"rope_base\": 1_000_000.0,\n        \"dtype\": torch.bfloat16,\n    }\nelif MODEL_SIZE == \"32B\":\n    CONFIG = {\n        \"vocab_size\": 151_936,\n        \"context_length\": 40_960,\n        \"emb_dim\": 5120,\n        \"n_heads\": 64,             # 60% more heads than 14B\n        \"n_layers\": 64,            # 60% more layers\n        \"hidden_dim\": 25600,       # 47% bigger ff\n        \"head_dim\": 128,\n        \"qk_norm\": True,\n        \"n_kv_groups\": 8,\n        \"rope_base\": 1_000_000.0,\n        \"dtype\": torch.bfloat16,\n    }\nelse:\n    raise ValueError(f\"Model size {MODEL_SIZE} not supported\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:18.354209Z","iopub.execute_input":"2025-07-29T13:08:18.354378Z","iopub.status.idle":"2025-07-29T13:08:18.373160Z","shell.execute_reply.started":"2025-07-29T13:08:18.354363Z","shell.execute_reply":"2025-07-29T13:08:18.372426Z"}},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":"### create and test the model with dummy input and print parameter counts","metadata":{}},{"cell_type":"code","source":"# create and test model\ntorch.manual_seed(123)  # for reproducible results\nmodel = Qwen3Model(CONFIG)\nprint(f\"Model created: {model}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:18.373794Z","iopub.execute_input":"2025-07-29T13:08:18.374000Z","iopub.status.idle":"2025-07-29T13:08:29.117511Z","shell.execute_reply.started":"2025-07-29T13:08:18.373977Z","shell.execute_reply":"2025-07-29T13:08:29.116752Z"}},"outputs":[{"name":"stdout","text":"Model created: Qwen3Model(\n  (tok_emb): Embedding(151936, 1024)\n  (layers): ModuleList(\n    (0-27): 28 x TransformerBlock(\n      (attn): GroupedQueryAttention(\n        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n        (q_norm): RMSNorm()\n        (k_norm): RMSNorm()\n      )\n      (ff): FeedForward(\n        (gate): Linear(in_features=1024, out_features=3072, bias=False)\n        (up): Linear(in_features=1024, out_features=3072, bias=False)\n        (down): Linear(in_features=3072, out_features=1024, bias=False)\n      )\n      (norm1): RMSNorm()\n      (norm2): RMSNorm()\n    )\n  )\n  (final_norm): RMSNorm()\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# test forward pass with dummy input\ntest_output = model(torch.tensor([1, 2, 3]).unsqueeze(0))\nprint(f\"Test output shape: {test_output.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:29.118331Z","iopub.execute_input":"2025-07-29T13:08:29.118598Z","iopub.status.idle":"2025-07-29T13:08:29.400219Z","shell.execute_reply.started":"2025-07-29T13:08:29.118573Z","shell.execute_reply":"2025-07-29T13:08:29.399544Z"}},"outputs":[{"name":"stdout","text":"Test output shape: torch.Size([1, 3, 151936])\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# calculate parameter counts\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:29.401138Z","iopub.execute_input":"2025-07-29T13:08:29.401399Z","iopub.status.idle":"2025-07-29T13:08:29.406510Z","shell.execute_reply.started":"2025-07-29T13:08:29.401371Z","shell.execute_reply":"2025-07-29T13:08:29.405885Z"}},"outputs":[{"name":"stdout","text":"Total parameters: 751,632,384\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# account for weight tying between embedding and lm_head\nunique_params = total_params - model.tok_emb.weight.numel()  \nprint(f\"Unique parameters: {unique_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:29.407175Z","iopub.execute_input":"2025-07-29T13:08:29.407372Z","iopub.status.idle":"2025-07-29T13:08:29.423339Z","shell.execute_reply.started":"2025-07-29T13:08:29.407356Z","shell.execute_reply":"2025-07-29T13:08:29.422616Z"}},"outputs":[{"name":"stdout","text":"Unique parameters: 596,049,920\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"### Estimate total GPU memory needed for model parameters, gradients, and buffers","metadata":{}},{"cell_type":"code","source":"def calc_memory_usage(model, dtype=torch.float32):\n    # calculates model memory requirements\n    total_params = 0\n    total_grads = 0\n    \n    for param in model.parameters():\n        param_count = param.numel()\n        total_params += param_count\n        \n        # add gradient memory if param requires grad\n        if param.requires_grad:\n            total_grads += param_count\n    \n    # add buffer memory (non-trainable tensors)\n    total_buffers = sum(buf.numel() for buf in model.buffers())\n    \n    # bytes per element for given dtype\n    bytes_per_elem = torch.tensor(0, dtype=dtype).element_size()\n    total_bytes = (total_params + total_grads + total_buffers) * bytes_per_elem\n    \n    # convert to GB\n    total_gb = total_bytes / (1024**3)\n    return total_gb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:29.424158Z","iopub.execute_input":"2025-07-29T13:08:29.424649Z","iopub.status.idle":"2025-07-29T13:08:29.439527Z","shell.execute_reply.started":"2025-07-29T13:08:29.424620Z","shell.execute_reply":"2025-07-29T13:08:29.438901Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"### Display memory usage for different precisions and move model to available device (GPU, MPS, or CPU)","metadata":{}},{"cell_type":"code","source":"# memory usage for different precisions\nprint(f\"\\nMemory usage:\")\nprint(f\"float32: {calc_memory_usage(model, torch.float32):.2f} GB\")\nprint(f\"bfloat16: {calc_memory_usage(model, torch.bfloat16):.2f} GB\")\nprint(f\"float16: {calc_memory_usage(model, torch.float16):.2f} GB\")\n\n# device selection and model loading\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using CUDA GPU\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\") \n    print(\"Using Apple MPS\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")\n\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:29.440435Z","iopub.execute_input":"2025-07-29T13:08:29.440733Z","iopub.status.idle":"2025-07-29T13:08:29.855727Z","shell.execute_reply.started":"2025-07-29T13:08:29.440682Z","shell.execute_reply":"2025-07-29T13:08:29.855081Z"}},"outputs":[{"name":"stdout","text":"\nMemory usage:\nfloat32: 5.64 GB\nbfloat16: 2.82 GB\nfloat16: 2.82 GB\nUsing CUDA GPU\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"Qwen3Model(\n  (tok_emb): Embedding(151936, 1024)\n  (layers): ModuleList(\n    (0-27): 28 x TransformerBlock(\n      (attn): GroupedQueryAttention(\n        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n        (q_norm): RMSNorm()\n        (k_norm): RMSNorm()\n      )\n      (ff): FeedForward(\n        (gate): Linear(in_features=1024, out_features=3072, bias=False)\n        (up): Linear(in_features=1024, out_features=3072, bias=False)\n        (down): Linear(in_features=3072, out_features=1024, bias=False)\n      )\n      (norm1): RMSNorm()\n      (norm2): RMSNorm()\n    )\n  )\n  (final_norm): RMSNorm()\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":57},{"cell_type":"markdown","source":"### Load pretrained weights from checkpoint dict into the model with shape checks and support for weight tying","metadata":{}},{"cell_type":"code","source":"def load_pretrained_weights(model, config, weights_dict):\n    # loads weights from huggingface checkpoint into our model\n    def assign_weight(left, right, name=\"unknown\"):\n        if left.shape != right.shape:\n            raise ValueError(f\"Shape mismatch for {name}: {left.shape} vs {right.shape}\")\n        return torch.nn.Parameter(right.clone().detach() if isinstance(right, torch.Tensor) else torch.tensor(right))\n\n    # load embedding weights\n    model.tok_emb.weight = assign_weight(\n        model.tok_emb.weight, \n        weights_dict[\"model.embed_tokens.weight\"], \n        \"embedding\"\n    )\n\n    # load transformer layer weights\n    for layer_idx in range(config[\"n_layers\"]):\n        block = model.layers[layer_idx] \n        attn = block.attn\n        \n        # attention projection weights\n        attn.q_proj.weight = assign_weight(\n            attn.q_proj.weight,\n            weights_dict[f\"model.layers.{layer_idx}.self_attn.q_proj.weight\"],\n            f\"layer_{layer_idx}_q_proj\"\n        )\n        attn.k_proj.weight = assign_weight(\n            attn.k_proj.weight,\n            weights_dict[f\"model.layers.{layer_idx}.self_attn.k_proj.weight\"],\n            f\"layer_{layer_idx}_k_proj\"\n        )\n        attn.v_proj.weight = assign_weight(\n            attn.v_proj.weight,\n            weights_dict[f\"model.layers.{layer_idx}.self_attn.v_proj.weight\"],\n            f\"layer_{layer_idx}_v_proj\"\n        )\n        attn.out_proj.weight = assign_weight(\n            attn.out_proj.weight,\n            weights_dict[f\"model.layers.{layer_idx}.self_attn.o_proj.weight\"],\n            f\"layer_{layer_idx}_out_proj\"\n        )\n        \n        # qk normalization weights if they exist\n        if hasattr(attn, \"q_norm\") and attn.q_norm is not None:\n            attn.q_norm.weight = assign_weight(\n                attn.q_norm.weight,\n                weights_dict[f\"model.layers.{layer_idx}.self_attn.q_norm.weight\"],\n                f\"layer_{layer_idx}_q_norm\"\n            )\n        if hasattr(attn, \"k_norm\") and attn.k_norm is not None:\n            attn.k_norm.weight = assign_weight(\n                attn.k_norm.weight,\n                weights_dict[f\"model.layers.{layer_idx}.self_attn.k_norm.weight\"],\n                f\"layer_{layer_idx}_k_norm\"\n            )\n        \n        # layer normalization weights\n        block.norm1.weight = assign_weight(\n            block.norm1.weight,\n            weights_dict[f\"model.layers.{layer_idx}.input_layernorm.weight\"],\n            f\"layer_{layer_idx}_norm1\"\n        )\n        block.norm2.weight = assign_weight(\n            block.norm2.weight,\n            weights_dict[f\"model.layers.{layer_idx}.post_attention_layernorm.weight\"],\n            f\"layer_{layer_idx}_norm2\"\n        )\n        \n        # feedforward weights\n        block.ff.gate.weight = assign_weight(\n            block.ff.gate.weight,\n            weights_dict[f\"model.layers.{layer_idx}.mlp.gate_proj.weight\"],\n            f\"layer_{layer_idx}_ff_gate\"\n        )\n        block.ff.up.weight = assign_weight(\n            block.ff.up.weight,\n            weights_dict[f\"model.layers.{layer_idx}.mlp.up_proj.weight\"],\n            f\"layer_{layer_idx}_ff_up\"\n        )\n        block.ff.down.weight = assign_weight(\n            block.ff.down.weight,\n            weights_dict[f\"model.layers.{layer_idx}.mlp.down_proj.weight\"],\n            f\"layer_{layer_idx}_ff_down\"\n        )\n    \n    # final layer norm and output head\n    model.final_norm.weight = assign_weight(\n        model.final_norm.weight, \n        weights_dict[\"model.norm.weight\"], \n        \"final_norm\"\n    )\n    \n    if \"lm_head.weight\" in weights_dict:\n        model.lm_head.weight = assign_weight(\n            model.lm_head.weight, \n            weights_dict[\"lm_head.weight\"], \n            \"lm_head\"\n        )\n    else:\n        # weight tying: reuse embedding weights for output\n        print(\"Using weight tying for output head\")\n        model.lm_head.weight = assign_weight(\n            model.lm_head.weight, \n            weights_dict[\"model.embed_tokens.weight\"], \n            \"lm_head_tied\"\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:29.856538Z","iopub.execute_input":"2025-07-29T13:08:29.856797Z","iopub.status.idle":"2025-07-29T13:08:29.866598Z","shell.execute_reply.started":"2025-07-29T13:08:29.856779Z","shell.execute_reply":"2025-07-29T13:08:29.865927Z"}},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":"### Download pretrained model weights from Hugging Face hub based on selected model size and mode (reasoning/base)","metadata":{}},{"cell_type":"code","source":"# download and load model weights\nimport json\nimport os\nfrom pathlib import Path\nfrom safetensors.torch import load_file\nfrom huggingface_hub import hf_hub_download, snapshot_download\n\nif USE_REASONING:\n    repo_id = f\"Qwen/Qwen3-{MODEL_SIZE}\"\nelse:\n    repo_id = f\"Qwen/Qwen3-{MODEL_SIZE}-Base\"\n\nlocal_dir = Path(repo_id).parts[-1]  # extract folder name\n\nprint(f\"Downloading weights from {repo_id}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:29.867356Z","iopub.execute_input":"2025-07-29T13:08:29.867659Z","iopub.status.idle":"2025-07-29T13:08:29.885347Z","shell.execute_reply.started":"2025-07-29T13:08:29.867617Z","shell.execute_reply":"2025-07-29T13:08:29.884769Z"}},"outputs":[{"name":"stdout","text":"Downloading weights from Qwen/Qwen3-0.6B...\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"# Download and Load Model Weights (Handling Sharded or Single File)","metadata":{}},{"cell_type":"code","source":"if MODEL_SIZE == \"0.6B\":\n    # small model has single safetensors file\n    weights_file = hf_hub_download(\n        repo_id=repo_id,\n        filename=\"model.safetensors\",\n        local_dir=local_dir,\n    )\n    weights = load_file(weights_file)\nelse:\n    # larger models are sharded across multiple files\n    repo_dir = snapshot_download(repo_id=repo_id, local_dir=local_dir)\n    index_file = os.path.join(repo_dir, \"model.safetensors.index.json\")\n    \n    with open(index_file, \"r\") as f:\n        index = json.load(f)\n    \n    weights = {}\n    # load all shard files and combine\n    for filename in set(index[\"weight_map\"].values()):\n        shard_path = os.path.join(repo_dir, filename)\n        shard = load_file(shard_path)\n        weights.update(shard)\n\nload_pretrained_weights(model, CONFIG, weights)\nmodel.to(device)\ndel weights  # free memory\n\nprint(\"Model weights loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:29.886179Z","iopub.execute_input":"2025-07-29T13:08:29.886423Z","iopub.status.idle":"2025-07-29T13:08:31.191116Z","shell.execute_reply.started":"2025-07-29T13:08:29.886402Z","shell.execute_reply":"2025-07-29T13:08:31.190421Z"}},"outputs":[{"name":"stdout","text":"Model weights loaded successfully!\n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"# Tokenizer Class for Text-Token Conversion with Chat Formatting","metadata":{}},{"cell_type":"code","source":"# tokenizer implementation\nfrom tokenizers import Tokenizer\n\nclass Qwen3Tokenizer():\n    # handles text <-> token conversion for qwen models\n    def __init__(self, tokenizer_path=\"tokenizer.json\", repo_id=None, add_gen_prompt=False, add_thinking=False):\n        self.tokenizer_path = tokenizer_path\n        self.add_gen_prompt = add_gen_prompt\n        self.add_thinking = add_thinking\n        \n        # download tokenizer if not found locally\n        tokenizer_file = Path(tokenizer_path)\n        if not tokenizer_file.is_file() and repo_id is not None:\n            _ = hf_hub_download(\n                repo_id=repo_id,\n                filename=str(tokenizer_file.name),\n                local_dir=str(tokenizer_file.parent.name)\n            )\n        \n        self.tokenizer = Tokenizer.from_file(tokenizer_path)\n\n    def encode(self, text):\n        # convert text to tokens using chat format\n        messages = [{\"role\": \"user\", \"content\": text}]\n        formatted = self.format_chat(\n            messages,\n            add_gen_prompt=self.add_gen_prompt,\n            add_thinking=self.add_thinking\n        )\n        return self.tokenizer.encode(formatted).ids\n\n    def decode(self, token_ids):\n        # convert tokens back to text\n        return self.tokenizer.decode(token_ids, skip_special_tokens=False)\n\n    @staticmethod\n    def format_chat(messages, add_gen_prompt=False, add_thinking=False):\n        # formats messages into qwen chat template\n        prompt = \"\"\n        for msg in messages:\n            prompt += f\"<|im_start|>{msg['role']}\\n{msg['content']}<|im_end|>\\n\"\n        \n        if add_gen_prompt:\n            prompt += \"<|im_start|>assistant\"\n            if not add_thinking:\n                prompt += \"<|think>\\n\\n<|/think>\\n\\n\"  # reasoning markers\n            else:\n                prompt += \"\\n\"\n        return prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:31.191935Z","iopub.execute_input":"2025-07-29T13:08:31.192223Z","iopub.status.idle":"2025-07-29T13:08:31.199530Z","shell.execute_reply.started":"2025-07-29T13:08:31.192177Z","shell.execute_reply":"2025-07-29T13:08:31.198762Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"# Initialize Tokenizer with Optional Reasoning Prompts","metadata":{}},{"cell_type":"code","source":"# setup tokenizer\nif USE_REASONING:\n    tokenizer_path = f\"Qwen3-{MODEL_SIZE}/tokenizer.json\"\nelse:\n    tokenizer_path = f\"Qwen3-{MODEL_SIZE}-Base/tokenizer.json\"\n\ntokenizer = Qwen3Tokenizer(\n    tokenizer_path=tokenizer_path,\n    repo_id=repo_id,\n    add_gen_prompt=USE_REASONING,\n    add_thinking=USE_REASONING\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:31.200259Z","iopub.execute_input":"2025-07-29T13:08:31.200801Z","iopub.status.idle":"2025-07-29T13:08:31.698151Z","shell.execute_reply.started":"2025-07-29T13:08:31.200782Z","shell.execute_reply":"2025-07-29T13:08:31.697264Z"}},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"# Test Tokenization and Decoding with Sample Prompt","metadata":{}},{"cell_type":"code","source":"# test tokenization\ntest_prompt = \"Give me a short introduction to large language models.\"\ntoken_ids = tokenizer.encode(test_prompt)\ndecoded_text = tokenizer.decode(token_ids)\nprint(f\"Test tokenization:\")\nprint(f\"Original: {test_prompt}\")\nprint(f\"Tokens: {token_ids[:10]}... (showing first 10)\")\nprint(f\"Decoded: {decoded_text[:100]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:31.701759Z","iopub.execute_input":"2025-07-29T13:08:31.701997Z","iopub.status.idle":"2025-07-29T13:08:31.718860Z","shell.execute_reply.started":"2025-07-29T13:08:31.701977Z","shell.execute_reply":"2025-07-29T13:08:31.718143Z"}},"outputs":[{"name":"stdout","text":"Test tokenization:\nOriginal: Give me a short introduction to large language models.\nTokens: [151644, 872, 198, 35127, 752, 264, 2805, 16800, 311, 3460]... (showing first 10)\nDecoded: <|im_start|>user\nGive me a short introduction to large language models.<|im_end|>\n<|im_start|>assist...\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"# Text Generation Function with Optional Sampling and Stopping Criteria","metadata":{}},{"cell_type":"code","source":"def generate_text(model, token_ids, max_tokens=150, context_size=None, temp=0.0, top_k=None, eos_id=None):\n    # generates text by predicting next tokens one by one\n    if context_size is None:\n        context_size = model.cfg[\"context_length\"]\n    \n    for _ in range(max_tokens):\n        # only use last context_size tokens to avoid memory issues\n        context_ids = token_ids[:, -context_size:]\n        \n        with torch.no_grad():\n            logits = model(context_ids)\n            next_logits = logits[:, -1, :]  # only care about last position\n        \n        # apply top-k filtering if specified\n        if top_k is not None:\n            top_vals, _ = torch.topk(next_logits, top_k)\n            min_val = top_vals[:, -1]\n            next_logits = torch.where(\n                next_logits < min_val, \n                torch.tensor(-torch.inf).to(next_logits.device), \n                next_logits\n            )\n        \n        # apply temperature and sample\n        if temp > 0.0:\n            next_logits = next_logits / temp\n            probs = torch.softmax(next_logits, dim=-1)\n            next_id = torch.multinomial(probs, num_samples=1)\n        else:\n            # greedy decoding\n            next_id = torch.argmax(next_logits, dim=-1, keepdim=True)\n        \n        # stop if eos token encountered\n        if eos_id is not None and next_id.item() == eos_id:\n            break\n            \n        # append new token\n        token_ids = torch.cat((token_ids, next_id), dim=1)\n    \n    return token_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:31.719749Z","iopub.execute_input":"2025-07-29T13:08:31.719969Z","iopub.status.idle":"2025-07-29T13:08:31.734263Z","shell.execute_reply.started":"2025-07-29T13:08:31.719953Z","shell.execute_reply":"2025-07-29T13:08:31.733504Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"# Generate Text with Timing and Performance Metrics","metadata":{}},{"cell_type":"code","source":"# generate text and measure performance\nimport time\n\nprint(\"\\nGenerating text...\")\ntorch.manual_seed(123)  # reproducible generation\n\nstart_time = time.time()\n\noutput_ids = generate_text(\n    model=model,\n    token_ids=torch.tensor(token_ids, device=device).unsqueeze(0),\n    max_tokens=150,\n    context_size=CONFIG[\"context_length\"],\n    top_k=1,  # greedy\n    temp=0.0\n)\n\ngen_time = time.time() - start_time\noutput_text = tokenizer.decode(output_ids.squeeze(0).tolist())\n\nprint(f\"Generation time: {gen_time:.2f} seconds\")\nprint(f\"Tokens generated: {output_ids.shape[1] - len(token_ids)}\")\nprint(f\"Tokens per second: {(output_ids.shape[1] - len(token_ids)) / gen_time:.1f}\")\n\nif torch.cuda.is_available():\n    max_memory = torch.cuda.max_memory_allocated() / (1024**3)\n    print(f\"Peak GPU memory: {max_memory:.2f} GB\")\n\nprint(f\"\\n\\nGenerated text:\\n{output_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:31.735382Z","iopub.execute_input":"2025-07-29T13:08:31.735620Z","iopub.status.idle":"2025-07-29T13:08:42.587991Z","shell.execute_reply.started":"2025-07-29T13:08:31.735594Z","shell.execute_reply":"2025-07-29T13:08:42.587237Z"}},"outputs":[{"name":"stdout","text":"\nGenerating text...\nGeneration time: 10.83 seconds\nTokens generated: 150\nTokens per second: 13.8\nPeak GPU memory: 9.83 GB\n\n\nGenerated text:\n<|im_start|>user\nGive me a short introduction to large language models.<|im_end|>\n<|im_start|>assistant\n<think>\nOkay, the user wants a short introduction to large language models. Let me start by recalling what I know. Large language models are AI systems that can understand and generate human language. They're trained on massive datasets, so they can learn complex patterns and nuances.\n\nI should mention their ability to understand and generate text, not just specific tasks. Maybe include examples like chatbots or content generation. Also, emphasize their adaptability and efficiency. Oh, and maybe touch on their applications in various fields. Let me check if I'm covering all key points without being too technical. Keep it concise, around 3-4 sentences. Make sure it's clear and easy to understand.\n</think>\n\nLarge language models (LLMs) are AI systems designed\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"# 1. parameter breakdown by component","metadata":{}},{"cell_type":"code","source":"def analyze_model_components(model):\n    component_params = {}\n    \n    # embedding layer\n    emb_params = model.tok_emb.weight.numel()\n    component_params['Token Embedding'] = emb_params\n    \n    # transformer layers\n    layer_params = 0\n    for layer in model.layers:\n        layer_params += sum(p.numel() for p in layer.parameters())\n    component_params['Transformer Layers'] = layer_params\n    \n    # final components\n    final_norm_params = sum(p.numel() for p in model.final_norm.parameters())\n    lm_head_params = model.lm_head.weight.numel()\n    \n    component_params['Final Norm'] = final_norm_params\n    component_params['LM Head'] = lm_head_params\n    \n    return component_params\n\ncomponents = analyze_model_components(model)\ntotal = sum(components.values())\n\nprint(\"\\nParameter Breakdown:\")\nfor component, count in components.items():\n    percentage = (count / total) * 100\n    print(f\"  {component}: {count:,} ({percentage:.1f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:42.588951Z","iopub.execute_input":"2025-07-29T13:08:42.589731Z","iopub.status.idle":"2025-07-29T13:08:42.596325Z","shell.execute_reply.started":"2025-07-29T13:08:42.589682Z","shell.execute_reply":"2025-07-29T13:08:42.595596Z"}},"outputs":[{"name":"stdout","text":"\nParameter Breakdown:\n  Token Embedding: 155,582,464 (20.7%)\n  Transformer Layers: 440,466,432 (58.6%)\n  Final Norm: 1,024 (0.0%)\n  LM Head: 155,582,464 (20.7%)\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"# 2. memory analysis for different batch sizes","metadata":{}},{"cell_type":"code","source":"def memory_analysis(model, seq_len=1024):\n    print(f\"\\nMemory Analysis (sequence length: {seq_len}):\")\n    \n    batch_sizes = [1, 2, 4, 8, 16, 32]\n    for batch_size in batch_sizes:\n        if torch.cuda.is_available():\n            torch.cuda.reset_peak_memory_stats()\n            \n        dummy_input = torch.randint(0, 1000, (batch_size, seq_len), device=device)\n        \n        with torch.no_grad():\n            _ = model(dummy_input)\n            \n        if torch.cuda.is_available():\n            memory_gb = torch.cuda.max_memory_allocated() / (1024**3)\n            print(f\"  Batch size {batch_size:2d}: {memory_gb:.2f} GB\")\n\nmemory_analysis(model, seq_len=512)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:42.597131Z","iopub.execute_input":"2025-07-29T13:08:42.597374Z","iopub.status.idle":"2025-07-29T13:08:50.569336Z","shell.execute_reply.started":"2025-07-29T13:08:42.597346Z","shell.execute_reply":"2025-07-29T13:08:50.568687Z"}},"outputs":[{"name":"stdout","text":"\nMemory Analysis (sequence length: 512):\n  Batch size  1: 4.43 GB\n  Batch size  2: 4.72 GB\n  Batch size  4: 5.16 GB\n  Batch size  8: 6.03 GB\n  Batch size 16: 7.78 GB\n  Batch size 32: 11.27 GB\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"# 3. inference speed benchmarks","metadata":{}},{"cell_type":"code","source":"def speed_benchmark(model, tokenizer):\n    print(f\"\\nSpeed Benchmarks:\")\n    \n    test_prompts = [\n        \"What is machine learning?\",\n        \"Explain quantum computing in simple terms.\",\n        \"Write a short story about a robot.\",\n        \"List the benefits of renewable energy.\"\n    ]\n    \n    total_time = 0\n    total_tokens = 0\n    \n    for i, prompt in enumerate(test_prompts):\n        input_ids = tokenizer.encode(prompt)\n        start = time.time()\n        \n        output = generate_text(\n            model=model,\n            token_ids=torch.tensor(input_ids, device=device).unsqueeze(0),\n            max_tokens=50,\n            temp=0.0\n        )\n        \n        elapsed = time.time() - start\n        tokens_generated = output.shape[1] - len(input_ids)\n        \n        total_time += elapsed\n        total_tokens += tokens_generated\n        \n        print(f\"  Prompt {i+1}: {tokens_generated} tokens in {elapsed:.2f}s ({tokens_generated/elapsed:.1f} tok/s)\")\n    \n    print(f\"  Average: {total_tokens/total_time:.1f} tokens/second\")\n\nspeed_benchmark(model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:08:50.572352Z","iopub.execute_input":"2025-07-29T13:08:50.572548Z","iopub.status.idle":"2025-07-29T13:09:04.473972Z","shell.execute_reply.started":"2025-07-29T13:08:50.572531Z","shell.execute_reply":"2025-07-29T13:09:04.473227Z"}},"outputs":[{"name":"stdout","text":"\nSpeed Benchmarks:\n  Prompt 1: 50 tokens in 4.06s (12.3 tok/s)\n  Prompt 2: 50 tokens in 3.28s (15.3 tok/s)\n  Prompt 3: 50 tokens in 3.28s (15.2 tok/s)\n  Prompt 4: 50 tokens in 3.28s (15.3 tok/s)\n  Average: 14.4 tokens/second\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"# 4. model configuration comparison","metadata":{}},{"cell_type":"code","source":"def compare_model_configs():\n    print(f\"\\nModel Size Comparison:\")\n    \n    configs = {\n        \"0.6B\": {\"emb_dim\": 1024, \"n_heads\": 16, \"n_layers\": 28, \"hidden_dim\": 3072},\n        \"1.7B\": {\"emb_dim\": 2048, \"n_heads\": 16, \"n_layers\": 28, \"hidden_dim\": 6144},\n        \"4B\": {\"emb_dim\": 2560, \"n_heads\": 32, \"n_layers\": 36, \"hidden_dim\": 9728},\n        \"8B\": {\"emb_dim\": 4096, \"n_heads\": 32, \"n_layers\": 36, \"hidden_dim\": 12288},\n        \"14B\": {\"emb_dim\": 5120, \"n_heads\": 40, \"n_layers\": 40, \"hidden_dim\": 17408},\n        \"32B\": {\"emb_dim\": 5120, \"n_heads\": 64, \"n_layers\": 64, \"hidden_dim\": 25600},\n    }\n    \n    print(f\"{'Size':<6} {'Emb Dim':<8} {'Heads':<6} {'Layers':<7} {'FF Dim':<8} {'Est Params':<12}\")\n    print(\"-\" * 55)\n    \n    for size, cfg in configs.items():\n        # rough parameter estimate\n        vocab_size = 151_936\n        emb_params = vocab_size * cfg[\"emb_dim\"]\n        layer_params = cfg[\"n_layers\"] * (\n            4 * cfg[\"emb_dim\"]**2 +  # attention projections (rough)\n            3 * cfg[\"emb_dim\"] * cfg[\"hidden_dim\"]  # feedforward\n        )\n        est_params = (emb_params + layer_params) / 1e9  # in billions\n        \n        print(f\"{size:<6} {cfg['emb_dim']:<8} {cfg['n_heads']:<6} {cfg['n_layers']:<7} {cfg['hidden_dim']:<8} {est_params:.1f}B\")\n\ncompare_model_configs()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:09:04.476794Z","iopub.execute_input":"2025-07-29T13:09:04.476986Z","iopub.status.idle":"2025-07-29T13:09:04.483568Z","shell.execute_reply.started":"2025-07-29T13:09:04.476971Z","shell.execute_reply":"2025-07-29T13:09:04.482818Z"}},"outputs":[{"name":"stdout","text":"\nModel Size Comparison:\nSize   Emb Dim  Heads  Layers  FF Dim   Est Params  \n-------------------------------------------------------\n0.6B   1024     16     28      3072     0.5B\n1.7B   2048     16     28      6144     1.8B\n4B     2560     32     36      9728     4.0B\n8B     4096     32     36      12288    8.5B\n14B    5120     40     40      17408    15.7B\n32B    5120     64     64      25600    32.7B\n","output_type":"stream"}],"execution_count":69},{"cell_type":"markdown","source":"# 5. attention pattern analysis (simplified)","metadata":{}},{"cell_type":"code","source":"def analyze_attention_heads():\n    print(f\"\\nAttention Configuration:\")\n    print(f\"  Total attention heads: {CONFIG['n_heads']}\")\n    print(f\"  Key-Value groups: {CONFIG['n_kv_groups']}\")\n    print(f\"  Heads per KV group: {CONFIG['n_heads'] // CONFIG['n_kv_groups']}\")\n    print(f\"  Head dimension: {CONFIG['head_dim']}\")\n    print(f\"  Memory savings from GQA: {CONFIG['n_heads'] / CONFIG['n_kv_groups']:.1f}x\")\n\nanalyze_attention_heads()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T13:09:04.484323Z","iopub.execute_input":"2025-07-29T13:09:04.484637Z","iopub.status.idle":"2025-07-29T13:09:04.503611Z","shell.execute_reply.started":"2025-07-29T13:09:04.484617Z","shell.execute_reply":"2025-07-29T13:09:04.502943Z"}},"outputs":[{"name":"stdout","text":"\nAttention Configuration:\n  Total attention heads: 16\n  Key-Value groups: 8\n  Heads per KV group: 2\n  Head dimension: 128\n  Memory savings from GQA: 2.0x\n","output_type":"stream"}],"execution_count":70}]}